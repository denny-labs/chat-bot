A fully modular, production-grade academic chatbot built using FastAPI, spaCy, and scikit-learn.
Designed for answering academic queries through intent classification, entity extraction, and template-based responses.

ğŸš€ 1. Project Overview

This project implements a rule-augmented NLP system consisting of:

âœ” Intent Classification

Using TF-IDF + LinearSVC (sklearn) to classify user messages into one of 27 academic intents.

âœ” Entity Extraction

Using spaCy with a lightweight EntityRuler to detect academic subjects (e.g., machine learning, calculus, photosynthesis).

âœ” Response Engine

Uses intents.json to map each intent to response templates.

âœ” REST API

Built with FastAPI to serve predictions for web or mobile clients.

ğŸ§± 2. Final Project Directory Structure

chat-bot/
â”‚ main.py
â”‚ rout.py
â”‚ requirements.txt
â”‚ README.md
â”‚
â”œâ”€â”€ data/
â”‚     intents.json
â”‚
â”œâ”€â”€ models/
â”‚     intent_model.pkl
â”‚     vectorizer.pkl
â”‚     spacy_nlp/
â”‚
â””â”€â”€ src/
      nlu_trainer.py
      nlu_initializer.py
      nlu_pipeline.py
      services/
          chat_service.py

Each component is modular and replaceable.

ğŸ¯ 3. Initial Planning & Architecture

The system was originally designed around three NLP components:

1ï¸âƒ£ Intent Classifier

Detects user intent (greetings, ask_explanation, ask_formula, ask_difference, etc.)

Built using scikit-learn LinearSVC + TF-IDF.

2ï¸âƒ£ NER Model

Detects subjects/topics in the question

Implemented with spaCy + EntityRuler

Detects entities like:

machine learning

photosynthesis

calculus

3ï¸âƒ£ Response Engine

Uses templates inside intents.json

Future versions can generate dynamic academic responses.

This design ensures a highly modular system where each component can be improved independently.

ğŸ“ 4. Dataset Preparation (PHASE 1)

The dataset is stored in:

data/intents.json

Format:

{
  "intents": [
    {
      "intent": "greetings",
      "patterns": ["hi", "hello", ...],
      "responses": ["Hello! How can I help you today?", ...]
    },
    ...
  ]
}

Dataset Highlights:

âœ” 27 total intents
âœ” ~30 patterns per intent (900+ training samples)
âœ” Multiple response templates per intent
âœ” All academic categories:
definitions â€¢ explanations â€¢ differences â€¢ examples â€¢ formulas â€¢ comparison â€¢ advantages â€¢ disadvantages â€¢ etc.

This dataset is the foundation of your NLU engine.

ğŸ§  5. Model Training (PHASE A)

Training is performed using:

python src/nlu_trainer.py

The trainer performs:

Load all training examples

Train TF-IDF vectorizer

Train LinearSVC intent classifier

Create spaCy EntityRuler

Save all models into /models/

Generated models:

models/
â”‚ intent_model.pkl
â”‚ vectorizer.pkl
â””â”€â”€ spacy_nlp/

This step must be run before starting the API.

ğŸ§© 6. NLU Pipeline (PHASE B)

Two core components are created:

ğŸ”¹ nlu_initializer.py

Loads:

intent_model.pkl

vectorizer.pkl

spaCy NLP pipeline

Used by ChatService.

ğŸ”¹ nlu_pipeline.py

Performs:

âœ” Intent Classification

intent, confidence = predict_intent(text)

âœ” Entity Extraction

entities = extract_entities(text)

âœ” Combined NLU Output

{
  "intent": "ask_explanation",
  "confidence": 2.17,
  "entities": [("machine learning", "SUBJECT")]
}

This is the â€œbrainâ€ of the chatbot.

ğŸ’¬ 7. Chat Service Logic (PHASE C)

chat_service.py handles full message processing:

âœ” Runs NLU
âœ” Selects a response from intents.json
âœ” Formats final output

Example output:

{
  "sender": "bot",
  "intent": "ask_definition",
  "confidence": 1.85,
  "entities": [],
  "reply": "Here is the definition:"
}

This service keeps all chat-related logic isolated.

ğŸŒ 8. API Layer (PHASE D)

rout.py exposes a single POST route:

POST /chat

Supports both:

JSON

form-data

Example request:

{
  "message": "explain gravity",
  "sender": "user"
}

Example response:

{
  "intent": "ask_explanation",
  "confidence": 2.14,
  "entities": [],
  "reply": "Let me explain that:",
  "sender": "bot"
}

main.py starts the API using Uvicorn:

python main.py

ğŸ§ª 9. Testing & Validation (PHASE E)

Testing is done at multiple levels:

ğŸ”¹ E1: Test Intent Classification

clf.predict(vec.transform(["give an example of os"]))

Expected â†’ ask_examples

ğŸ”¹ E2: Test spaCy Entities

nlp("explain machine learning")

Expected â†’ [('machine learning', 'SUBJECT')]

ğŸ”¹ E3: Test NLUPipeline

nlu.run("difference between virus and bacteria")

ğŸ”¹ E4: Test ChatService

asyncio.run(chat.process_message("hi"))

ğŸ”¹ E5: Test API (Postman)

Send POST request to:

http://localhost:8000/chat

Everything should work end-to-end.

ğŸ“ˆ 10. Future Enhancements (PHASE F-G-H)
ğŸ”® Phase F â€” Dynamic Academic Answers

Currently, responses are templates.
Upgrade paths:

Add long-form explanations

Generate formulas dynamically

Add algorithm steps

Add code generation

Integrate with Wikipedia APIs

ğŸ–¥ï¸ Phase G â€” Frontend UI

You can build a simple web interface using:

HTML + JavaScript

React

Streamlit

Flutter

Frontend interacts with /chat API.

ğŸ§  Phase H â€” Smarter NER

Enhance NER by adding patterns for:

Topic

Programming languages

Theorems

Formulas

Algorithms

Even migrate to:

Custom spaCy NER

HuggingFace transformer-based NER

ğŸ§º 11. Additional Recommendations
âœ” Add CORS Middleware

If you plan to connect to frontend.

âœ” Add /ping health check

Debug quickly.

âœ” Add logging system

Better production monitoring.

âœ” Create a fallback knowledge retrieval module

Use Wikipedia or textbooks for deeper academic answers.

ğŸ 12. Conclusion

You now have:

âœ” Fully functional academic chatbot
âœ” End-to-end NLU pipeline
âœ” Clean FastAPI backend
âœ” Accurate intent classification
âœ” Basic NER support
âœ” Modular, extensible design

This project is structured to scale â€” you can easily expand intents, add more entities, or swap ML models with minimal changes.